{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "examples_dir = os.path.dirname('/notebooks/demon/examples/')\n",
    "weights_dir = os.path.join(examples_dir,'..','weights')\n",
    "sys.path.insert(0, os.path.join(examples_dir, '..', 'python'))\n",
    "\n",
    "sys.path.append('/notebooks/demon/python')\n",
    "sys.path.append('/notebooks/demon/lmbspecialops/python')\n",
    "\n",
    "from depthmotionnet.networks_original import *\n",
    "\n",
    "\n",
    "def prepare_input_data(img1, img2, data_format):\n",
    "    \"\"\"Creates the arrays used as input from the two images.\"\"\"\n",
    "    # scale images if necessary\n",
    "    if img1.size[0] != 256 or img1.size[1] != 192:\n",
    "        img1 = img1.resize((256,192))\n",
    "    if img2.size[0] != 256 or img2.size[1] != 192:\n",
    "        img2 = img2.resize((256,192))\n",
    "    img2_2 = img2.resize((64,48))\n",
    "        \n",
    "    # transform range from [0,255] to [-0.5,0.5]\n",
    "    img1_arr = np.array(img1).astype(np.float32)/255 -0.5\n",
    "    img2_arr = np.array(img2).astype(np.float32)/255 -0.5\n",
    "    img2_2_arr = np.array(img2_2).astype(np.float32)/255 -0.5\n",
    "    \n",
    "    if data_format == 'channels_first':\n",
    "        img1_arr = img1_arr.transpose([2,0,1])\n",
    "        img2_arr = img2_arr.transpose([2,0,1])\n",
    "        img2_2_arr = img2_2_arr.transpose([2,0,1])\n",
    "        image_pair = np.concatenate((img1_arr,img2_arr), axis=0)\n",
    "    else:\n",
    "        image_pair = np.concatenate((img1_arr,img2_arr),axis=-1)\n",
    "    \n",
    "    result = {\n",
    "        'image_pair': image_pair[np.newaxis,:],\n",
    "        'image1': img1_arr[np.newaxis,:], # first image\n",
    "        'image2_2': img2_2_arr[np.newaxis,:], # second image with (w=64,h=48)\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "if tf.test.is_gpu_available(True):\n",
    "    data_format='channels_first'\n",
    "else: # running on cpu requires channels_last data format\n",
    "    data_format='channels_last'\n",
    "\n",
    "# \n",
    "# DeMoN has been trained for specific internal camera parameters.\n",
    "#\n",
    "# If you use your own images try to adapt the intrinsics by cropping\n",
    "# to match the following normalized intrinsics:\n",
    "#\n",
    "#  K = (0.89115971  0           0.5)\n",
    "#      (0           1.18821287  0.5)\n",
    "#      (0           0           1  ),\n",
    "#  where K(1,1), K(2,2) are the focal lengths for x and y direction.\n",
    "#  and (K(1,3), K(2,3)) is the principal point.\n",
    "#  The parameters are normalized such that the image height and width is 1.\n",
    "#\n",
    "\n",
    "# read data\n",
    "img1 = Image.open(os.path.join(examples_dir,'sculpture1.png'))\n",
    "img2 = Image.open(os.path.join(examples_dir,'sculpture2.png'))\n",
    "\n",
    "input_data = prepare_input_data(img1,img2,data_format)\n",
    "\n",
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.per_process_gpu_memory_fraction=0.8\n",
    "session = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n",
    "\n",
    "# init networks\n",
    "bootstrap_net = BootstrapNet(session, data_format)\n",
    "iterative_net = IterativeNet(session, data_format)\n",
    "refine_net = RefinementNet(session, data_format)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# load weights\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session,os.path.join(weights_dir,'demon_original'))\n",
    "\n",
    "\n",
    "# run the network\n",
    "result = bootstrap_net.eval(input_data['image_pair'], input_data['image2_2'])\n",
    "for i in range(3):\n",
    "    result = iterative_net.eval(\n",
    "        input_data['image_pair'], \n",
    "        input_data['image2_2'], \n",
    "        result['predict_depth2'], \n",
    "        result['predict_normal2'], \n",
    "        result['predict_rotation'], \n",
    "        result['predict_translation']\n",
    "    )\n",
    "rotation = result['predict_rotation']\n",
    "translation = result['predict_translation']\n",
    "result = refine_net.eval(input_data['image1'],result['predict_depth2'])\n",
    "\n",
    "\n",
    "plt.imshow(result['predict_depth0'].squeeze(), cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "# try to visualize the point cloud\n",
    "try:\n",
    "    from depthmotionnet.vis import *\n",
    "    visualize_prediction(\n",
    "        inverse_depth=result['predict_depth0'], \n",
    "        image=input_data['image_pair'][0,0:3], \n",
    "        rotation=rotation, \n",
    "        translation=translation)\n",
    "except ImportError as err:\n",
    "    print(\"Cannot visualize as pointcloud.\", err)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
